{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract SQ_TP, SQ_FP and SQ_R papers from oa_comm database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import csv\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that searches for papers using our SQs and returns a list of PubMed IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pubmed_for_ids(query, max_results=13):\n",
    "    Entrez.email = \"zeynep.korkmaz@tum.de\"  # Set email address\n",
    "\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    return record[\"IdList\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that reads the keywords and SQs from a directory with CSV files and returns a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_keywords_from_directory(directory):\n",
    "    keywords_dict = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            csv_file = os.path.join(directory, filename)\n",
    "            \n",
    "            # for troubleshooting (dictionary only contains 99 files but should contain ~140)\n",
    "            #print(\"Reading keywords from file: {}\".format(csv_file)) # all files are read\n",
    "\n",
    "            with open(csv_file, 'r') as file:\n",
    "                reader = csv.reader(file)\n",
    "\n",
    "                current_pub_title = None\n",
    "                current_keywords = []\n",
    "                current_sq_tp = []\n",
    "                current_sq_fp = []\n",
    "                current_sq_r = []\n",
    "\n",
    "                for row in reader:\n",
    "                    row = [item.strip(', ') for item in row]\n",
    "                    if row and not row[0].isdigit():\n",
    "                        if row[0] == \"Pub Title\":\n",
    "                            if current_pub_title:\n",
    "                                keywords_dict[current_pub_title] = {\n",
    "                                    \"Pub Title\": current_pub_title,\n",
    "                                    \"Keywords\": current_keywords,\n",
    "                                    \"SQ_TP\": current_sq_tp,\n",
    "                                    \"SQ_FP\": current_sq_fp,\n",
    "                                    \"SQ_R\": current_sq_r\n",
    "                                }\n",
    "                            current_pub_title = row[1]\n",
    "                            current_keywords = []\n",
    "                            current_sq_tp = []\n",
    "                            current_sq_fp = []\n",
    "                            current_sq_r = []\n",
    "                        elif row[0] == \"Keywords\":\n",
    "                            current_keywords.extend(item for item in row[1:] if item)\n",
    "                        elif row[0] == \"SQ_TP\":\n",
    "                            current_sq_tp.extend(item for item in row[1:] if item)\n",
    "                        elif row[0] == \"SQ_FP\":\n",
    "                            current_sq_fp.extend(item for item in row[1:] if item)\n",
    "                        elif row[0] == \"SQ_R\":\n",
    "                            current_sq_r.extend(item for item in row[1:] if item)\n",
    "\n",
    "                if current_pub_title:\n",
    "                    keywords_dict[current_pub_title] = {\n",
    "                        \"Pub Title\": current_pub_title,\n",
    "                        \"Keywords\": current_keywords,\n",
    "                        \"SQ_TP\": current_sq_tp,\n",
    "                        \"SQ_FP\": current_sq_fp,\n",
    "                        \"SQ_R\": current_sq_r\n",
    "                    }\n",
    "\n",
    "    return keywords_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage of read_keywords_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to csv file \n",
    "input_dir = \"Keyword_CSVs\" \n",
    "#input_dir = \"less_keywords\" \n",
    "\n",
    "# create dictionary from csv\n",
    "keywords_dict = read_keywords_from_directory(input_dir)\n",
    "\n",
    "#print(\"\\n############# \\n\")\n",
    "\n",
    "# Why is this only 99? Should be ~140\n",
    "#print(len(keywords_dict))\n",
    "\n",
    "#print(\"\\n############# \\n\")\n",
    "\n",
    "# print dictionary\n",
    "#for pub_title, data in keywords_dict.items():\n",
    "          #  print(f\"Pub Title: {data['Pub Title']}\")\n",
    "          #  print(f\"Keywords: {', '.join(data['Keywords'])}\")\n",
    "          #  print(f\"SQ_TP: {', '.join(data['SQ_TP'])}\")\n",
    "          #  print(f\"SQ_FP: {', '.join(data['SQ_FP'])}\")\n",
    "          #  print(f\"SQ_R: {', '.join(data['SQ_R'])}\")\n",
    "           # print(\"\\n\" + \"=\" * 80 + \"\\n\")  # Separator between entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that takes keyword_dict/input_dict and returns dict with list of PubMed IDs based on SQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_pubmed_id(input_dict):\n",
    "    # Initialize a new dictionary to store the results\n",
    "    result_dict = {}\n",
    "\n",
    "    # Iterate over each publication entry in the input dictionary\n",
    "    for pub_title, pub_data in input_dict.items():\n",
    "        # Create a copy of the publication data\n",
    "        pub_result = pub_data.copy()\n",
    "\n",
    "        # Initialize empty lists for PubMed IDs for SQ_TP, SQ_FP, and SQ_R\n",
    "        pub_result['PubMed_IDs_TP'] = []\n",
    "        pub_result['PubMed_IDs_FP'] = []\n",
    "        pub_result['PubMed_IDs_R'] = []\n",
    "\n",
    "        # Extract elements from SQ_TP, SQ_FP, and SQ_R lists and search PubMed for IDs\n",
    "        for sq_tp_element in pub_data['SQ_TP']:\n",
    "            pub_result['PubMed_IDs_TP'].extend(search_pubmed_for_ids(sq_tp_element))\n",
    "\n",
    "        for sq_fp_element in pub_data['SQ_FP']:\n",
    "            pub_result['PubMed_IDs_FP'].extend(search_pubmed_for_ids(sq_fp_element))\n",
    "\n",
    "        for sq_r_element in pub_data['SQ_R']:\n",
    "            pub_result['PubMed_IDs_R'].extend(search_pubmed_for_ids(sq_r_element))\n",
    "\n",
    "        # Add the modified publication data to the result dictionary\n",
    "        result_dict[pub_title] = pub_result\n",
    "\n",
    "    return result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict_to_pubmed_id(keywords_dict)\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result_dict.items())\n",
    "for pub_title, data in result_dict.items():\n",
    "            print(data['PubMed_IDs_TP'])\n",
    "            #print(f\"Pub Title: {data['Pub Title']}\")\n",
    "            #print(f\"Keywords: {', '.join(data['Keywords'])}\")\n",
    "            #print(f\"SQ_TP: {', '.join(data['SQ_TP'])}\")\n",
    "            #print(f\"SQ_FP: {', '.join(data['SQ_FP'])}\")\n",
    "            #print(f\"SQ_R: {', '.join(data['SQ_R'])}\")\n",
    "            #print(f\"PubMed_IDs_TP: {', '.join(data['PubMed_IDs_TP'])}\")\n",
    "            #print(f\"PubMed_IDs_FP: {', '.join(data['PubMed_IDs_FP'])}\")\n",
    "            #print(f\"PubMed_IDs_R: {', '.join(data['PubMed_IDs_R'])}\")\n",
    "            #print(\"\\n\" + \"=\" * 80 + \"\\n\")  # Separator between entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that takes dict with list of PubMeds IDs for the SQs as input, searches specified directory for the corresping XML papers and combines all to one large XML file (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_xml_files(input_dict, input_dir, output_file):\n",
    "    \n",
    "    SQ_IDs = {\n",
    "        'SQ_TP_IDs': [id for pub_title, data in input_dict.items() for id in data['PubMed_IDs_TP']],\n",
    "        'SQ_FP_IDs': [id for pub_title, data in input_dict.items() for id in data['PubMed_IDs_FP']],\n",
    "        'SQ_R_IDs': [id for pub_title, data in input_dict.items() for id in data['PubMed_IDs_R']]\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        f.write(b'<root>\\n')\n",
    "\n",
    "        for SQ, desired_IDs in SQ_IDs.items():\n",
    "            SQ_root = ET.Element(f'{SQ}')\n",
    "\n",
    "            for root_dir, dirs, files in os.walk(input_dir):\n",
    "                for xml_file in files:\n",
    "                    if xml_file.endswith('.xml'):\n",
    "                        xml_file_path = os.path.join(root_dir, xml_file)\n",
    "                        \n",
    "                        try:\n",
    "                            tree = ET.parse(xml_file_path)\n",
    "                        except ET.ParseError:\n",
    "                            print(f\"Skipping file due to ParseError: {xml_file_path}\")\n",
    "                            continue\n",
    "\n",
    "                        root = tree.getroot()\n",
    "                        root_copy = copy.deepcopy(root)\n",
    "\n",
    "                        for element in root_copy.iter('article-id'):\n",
    "                            if element.attrib.get('pub-id-type') == 'pmid' and element.text in desired_IDs:\n",
    "                                SQ_root.append(root_copy)\n",
    "\n",
    "            f.write(ET.tostring(SQ_root, encoding='utf-8'))\n",
    "\n",
    "        f.write(b'</root>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/Users/tillohlendorf/Downloads/Extraced_XML'\n",
    "output_file = f'{os.path.join(os.getcwd(), \"output.xml\")}'\n",
    "\n",
    "\n",
    "extract_xml_files(result_dict, input_dir, output_file )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPforCRC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
